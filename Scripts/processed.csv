HITId,AssignmentId,WorkerId,WorkTimeInSeconds,Input.Id,Input.Venue,Input.Title,Input.Authors,Input.Cites,Input.Abstract,Citation_Chosen,Citation_EmptyCount,Citation_Same,Doi_Chosen,Doi_EmptyCount,Doi_Same,Methods?_Chosen,Methods?_EmptyCount,Methods?_Same,Paper-link_Chosen,Paper-link_EmptyCount,Paper-link_Same,Primary?_Chosen,Primary?_EmptyCount,Primary?_Same,Quant?_Chosen,Quant?_EmptyCount,Quant?_Same,Read?_Chosen,Read?_EmptyCount,Read?_Same,Suggestion_Chosen,Suggestion_EmptyCount,Suggestion_Same
_EmptyCount,Quant?_Same,Read?_Chosen,Read?_EmptyCount,Read?_Same,Suggestion_Chosen,Suggestion_EmptyCount,Suggestion_Same
30F94FBDNR8BT8OW1J7AVD3MNPOBTE,"['3FPRZHYEPZVNPPM2HNXK2WOIB9S3V5', '3LEP4MGT3HO6Y3QT5VCWETGNZUNBDR', '3QECW5O0KIPBQC5HQBLYGBYHT565TR']","['A3F9JBVNMQ4ZUV', 'A1665VCUSZL9YT', 'AZ72Z7VU6TQCN']","['134', '312', '93']",14345,15,Test input generation with java PathFinder,"willem visser,corina s. pasareanu,sarfraz khurshid","69
","We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java TreeMap library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.","['http://users.ece.utexas.edu/~khurshid/papers/JPF-issta04.pdf', 'https://doi.org/10.1145/1007512.1007526', 'doi>10.1145/1007512.1007526']","['1', '4', '1', '1', '2']","['1', '1', '1']","['1', '1', '1']","['1', '2', '1', '2', '2', '3']","['{}', '{}', '{}']","['486', '173', '487']","['http://dl.acm.org/citation.cfm?id=1007526', 'http://dl.acm.org/citation.cfm?id=1007526', 'http://dl.acm.org/citation.cfm?id=1007526']",0,0,0.5,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
30OITAWPBQR206I435CPQO58OVQ9HS,"['3NG53N1RLW7WL2VC7SJHNL7B57G8PE', '3P529IW9KZ9F9WQ1NEWGGWFDN14LFM', '3ZV9H2YQQEV04EM77X2GOY7WJ3XW3I']","['A3F9JBVNMQ4ZUV', 'A3KFX4FS1SANOS', 'AZ72Z7VU6TQCN']","['86', '443', '92']",10319,9,Yesterday My Program Worked Today It Does Not Why,andreas zeller,"47
","Imagine some program and a number of changes. If none of these changes is applied (yesterday), the program works. If all changes are applied (today), the program does not work. Which change is responsible for the failure? We present an efficient algorithm that determines the minimal set of failure-inducing changes. Our delta debugging prototype tracked down a single failure-inducing change from 178,000 changed GDB lines within a few hours.","['https://pdfs.semanticscholar.org/1a11/995cca0eb239a7b95d23b4a42c6a634fcf41.pdf', 'DOI: 10.1007/3-540-48166-4_16', 'DOI: 10.1007/3-540-48166-4_16']","['1', '4', '1', '2']","['1', '1', '1']","['1', '3', '1']","['2', '2', '2', '3']","['{}', 'NONE', '{}']","['344', '344', '345']","['https://link.springer.com/chapter/10.1007/3-540-48166-4_16', 'https://link.springer.com/chapter/10.1007/3-540-48166-4_16', 'https://link.springer.com/chapter/10.1007/3-540-48166-4_16']",1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,2,1
31HLTCK4BLJMXH0WM0Z3X6I40NWGVG,"['35USIKEBNS49E30BYY9RZI6PZ7XN63', '3A4NIXBJ77NLF6II910TBMZMK1EMLQ', '3X4MXAO0BHCSNG79IN3C6WACOXRRWQ']","['AZ72Z7VU6TQCN', 'A3F9JBVNMQ4ZUV', 'A1E9VI5RSZI31F']","['63', '99', '292']",1809,1,Predicting defects using network analysis on dependency graphs,"thomas zimmermann,nachiappan nagappan","55
","In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.","['DOI: 10.1145/1368088.1368161', 'http://thomas-zimmermann.com/publications/files/zimmermann-icse-2008.pdf', '10.1145/1368088.1368161']","['1', '1', '1']","['1', '1', '1']","['', '1', '1']","['2', '3', '2', '2']","['{}', '{}', '{}']","['382', '381', '120']","['http://ieeexplore.ieee.org/abstract/document/4814164/', 'http://ieeexplore.ieee.org/abstract/document/4814164/', 'http://ieeexplore.ieee.org/abstract/document/4814164/']",0,0,0.5,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
335VBRURDJOQGWR2D44J5L8VWFY9EC,"['3907X2AHF1TLZNTNRBAGDUU8JI62P9', '3EF8EXOTT2JIOR8TDU5IBFYRI1VJ1U']","['AZ72Z7VU6TQCN', 'A3F9JBVNMQ4ZUV']","['170', '189']",14374,15,Effectively prioritizing tests in development environment,"amitabh srivastava,jay thiagarajan","51
","Software testing helps ensure not only that the software under development has been implemented correctly, but also that further development does not break it. If developers introduce new defects into the software, these should be detected as early and inexpensively as possible in the development cycle. To help optimize which tests are run at what points in the design cycle, we have built Echelon, a test prioritization system, which prioritizes the application's given set of tests, based on what changes have been made to the program.Echelon builds on the previous work on test prioritization and proposes a practical binary code based approach that scales well to large systems. Echelon utilizes a binary matching system that can accurately compute the differences at a basic block granularity between two versions of the program in binary form. Echelon utilizes a fast, simple and intuitive heuristic that works well in practice to compute what tests will cover the affected basic blocks in the program. Echelon orders the given tests to maximally cover the affected program so that defects are likely to be found quickly and inexpensively. Although the primary focus in Echelon is on program changes, other criteria can be added in computing the priorities.Echelon is part of a test effectiveness infrastructure that runs under the Windows environment. It is currently being integrated into the Microsoft software development process. Echelon has been tested on large Microsoft product binaries. The results show that Echelon is effective in ordering tests based on changes between two program versions.","['doi>10.1145/566172.566187', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.526.8448&rep=rep1&type=pdf']","['1', '1']","['1', '1']","['1', '1']","['2', '2']","['{}', '{}']","['330', '330']","['http://dl.acm.org/citation.cfm?id=566187', 'http://dl.acm.org/citation.cfm?id=566187']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,2,1
335VBRURDJOQGWR2D44J5L8VWFYE9H,"['33UKMF931A8DBEG7R73WJR4V98OTT2', '3G5F9DBFOQL2JXT1W8MM6QPSUEDHVT', '3O6CYIULEEPK38ZQMHYHB7XWVDQWUS']","['A2VO6V2FJEQZZ', 'A3F9JBVNMQ4ZUV', 'AZ72Z7VU6TQCN']","['149', '79', '65']",20738,19,Analyzing Regression Test Selection Techniques,"gregg rothermel,mary jean harrold","67
","Regression testing is a necessary but expensive maintenance activity aimed at showing that code has not been adversely affected by changes. Regression test selection techniques reuse tests from an existing test suite to test a modified program. Many regression test selection techniques have been proposed; however, it is difficult to compare and evaluate these techniques because they have different goals. This paper outlines the issues relevant to regression test selection techniques, and uses these issues as the basis for a framework within which to evaluate the techniques. We illustrate the application of our framework by using it to evaluate existing regression test selection techniques. The evaluation reveals the strengths and weaknesses of existing techniques, and highlights some problems that future work in this area should address.","['http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1013&context=csearticles', 'http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1013&context=csearticles', 'DOI: 10.1109/32.536955']","['1', '', '1']","['1', '1', '1']","['1', '1', '1']","['1', '2', '3', '4', '2', '1', '2']","['{}', '{}', '{}']","['686', '685', '686']","['http://ieeexplore.ieee.org/abstract/document/536955/', 'http://ieeexplore.ieee.org/abstract/document/536955/', 'http://ieeexplore.ieee.org/abstract/document/536955/']",1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
33KGGVH24U5HXRFO4WFIE4BJE9KX1Z,"['3P529IW9KZ9F9WQ1NEWGGWFDN04FLE', '3WETL7AQWUW6RV7NBWF7YYUJY3353D']","['A3F9JBVNMQ4ZUV', 'AZ72Z7VU6TQCN']","['103', '86']",21185,19,Using Program Slicing in Software Maintenance,"keith gallagher,james r. lyle","62
","Program slicing is applied to the software maintenance problem by extending the notion of a program slice (that originally required both a variable and line number) to a decomposition slice, one that captures all computation on a given variable, i.e., is independent of line numbers. Using the lattice of single variable decomposition slices ordered by set inclusion, it is shown how a slice-based decomposition for programs can be formed. One can then delineate the effects of a proposed change by isolating those effects in a single component of the decomposition. This gives maintainers a straightforward technique for determining those statements and variables which may be modified in a component and those which may not. Using the decomposition, a set of principles to prohibit changes which will interfere with unmodified components is provided. These semantically consistent changes can then be merged back into the original program in linear time.","['http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.1532&rep=rep1&type=pdf', 'DOI: 10.1109/32.83912']","['1', '1']","['1', '1']","['1', '']","['2', '1', '2']","['{}', '{}']","['821', '821']","['http://ieeexplore.ieee.org/abstract/document/83912/', 'http://ieeexplore.ieee.org/abstract/document/83912/']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,0,0,0.5,1,0,1,1,2,1
33KGGVH24U5HXRFO4WFIE4BJE9L1X4,"['3BXQMRHWK0MOYW5CIFSU29V5XT3MUI', '3JPSL1DZ5TNA15CEOBQ3FN8BVQMANF', '3L0KT67Y8F48D02MCDL5H30OL5ASYA']","['A1665VCUSZL9YT', 'A314XJY8V1YL12', 'A3F9JBVNMQ4ZUV']","['303', '265', '706']",1866,1,FeedbackDirected Random Test Generation,"carlos pacheco,shuvendu k. lahiri,michael d. ernst,thomas a. ball","75
","We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.","['https://doi.org/10.1109/ICSE.2007.37', 'https://doi.org/10.1109/ICSE.2007.37', '10.1109/ICSE.2007.37']","['1', '1', '4']","['1', '1', '1']","['1', '1', '1']","['1', '2', '1', '2', '3', '1', '2']","['{}', '{}', '{}']","['193', '545', '545']","['http://dl.acm.org/citation.cfm?id=1248841', 'http://dl.acm.org/citation.cfm?id=1248841', 'http://dl.acm.org/citation.cfm?id=1248841']",1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
33N1S8XHHM962MF35E33BPRUM9J1Z1,"['3HMVI3QICKG8JG3SZU6LL2DFVHZY1M', '3MAOD8E57RYMXIK7ZLIHZVJ9W1MNX3', '3TY7ZAOG5G8DSMTTZW1UYPFUSY70KP']","['A2E8Q6UFDQLOJ3', 'AAZGEO8M5ZYOB', 'A3F9JBVNMQ4ZUV']","['313', '250', '86']",9869,9,Fair and balanced bias in bugfix datasets,"christian bird,adrian bachmann,eirik aune,john duffy,abraham bernstein,vladimir filkov,premkumar t. devanbu","57
","Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical data from several software projects, and find strong evidence of systematic bias. We then investigate the potential effects of ""unfair, imbalanced"" datasets on the performance of prediction techniques. We draw the lesson that bias is a critical problem that threatens both the effectiveness of processes that rely on biased datasets to build prediction models and the generalizability of hypotheses tested on biased data.","['https://doi.org/10.1145/1595696.1595716', '10.1145/1595696.1595716', 'http://cabird.com/pubs/bird2009fbb.pdf']","['3', '3', '4', '']","['1', '2', '1']","['1', '3', '2']","['2', '1', '2', '3', '4', '1', '2']","['{}', '{}', '{}']","['102', '102', '260']","['http://dl.acm.org/citation.cfm?id=1595716', 'http://dl.acm.org/citation.cfm?id=1595716', 'http://dl.acm.org/citation.cfm?id=1595716']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,0,0,0.5,1,0,1,1,3,1
33W1NHWFYH97B6RC7XNDHVAPES9ZT7,"['39RP059MEIH92XW6IOBZXJ3H8PIBMA', '3E4GGUZ1T9FKOWW7EJ58HD89PXO2KA']","['AZ72Z7VU6TQCN', 'A3F9JBVNMQ4ZUV']","['77', '59']",2178,1,Use of relative code churn measures to predict system defect density,"nachiappan nagappan,thomas a. ball","81
","Software systems evolve over time due to changes in requirements, optimization of code, fixes for security and reliability bugs etc. Code churn, which measures the changes made to a component over a period of time, quantifies the extent of this change. We present a technique for early prediction of system defect density using a set of relative code churn measures that relate the amount of churn to other variables such as component size and the temporal extent of churn.Using statistical regression models, we show that while absolute measures of code churn are poor predictors of defect density, our set of relative measures of code churn is highly predictive of defect density. A case study performed on Windows Server 2003 indicates the validity of the relative code churn measures as early indicators of system defect density. Furthermore, our code churn metric suite is able to discriminate between fault and not fault-prone binaries with an accuracy of 89.0 percent.","['DOI: 10.1109/ICSE.2005.1553571', 'http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.7712&rep=rep1&type=pdf']","['1', '3']","['1', '1']","['1', '1']","['2', '2']","['{}', '{}']","['545', '543']","['http://ieeexplore.ieee.org/abstract/document/1553571/', 'http://ieeexplore.ieee.org/abstract/document/1553571/']",0,0,0.5,0,0,0.5,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,2,1
34KYK9TV2RWAE8BHCJ2KL870L6ABSP,"['3HVVDCPGTFG9S0F11DQVR8WANO4YTL', '3QECW5O0KIPBQC5HQBLYGBYHT52T5B']","['A3F9JBVNMQ4ZUV', 'A2VO6V2FJEQZZ']","['117', '107']",2356,1,Mining Version Histories to Guide Software Changes,"thomas zimmermann,peter weissgerber,stephan diehl,andreas zeller","70
","We apply data mining to version histories in order toguide programmers along related changes: ""Programmerswho changed these functions also changed. . . "". Given aset of existing changes, such rules (a) suggest and predictlikely further changes, (b) show up item coupling that is indetectableby program analysis, and (c) prevent errors dueto incomplete changes. After an initial change, our ROSEprototype can correctly predict 26% of further files to bechanged  and 15% of the precise functions or variables.The topmost three suggestions contain a correct locationwith a likelihood of 64%.","['10.1109/TSE.2005.72', 'http://222.252.30.203:8888/bitstream/123456789/10409/1/429-455..Software%20Engineering,%20IEEE%20Transactions.%20Vol.31.Iss.6.A.2.pdf']","['1', '1']","['1', '1']","['1', '1']","['1', '2', '1', '2', '3']","['{}', '{}']","['1110', '1110']","['http://ieeexplore.ieee.org/abstract/document/1463228/', 'http://ieeexplore.ieee.org/abstract/document/1463228/?reload=true']",1,0,1,0,0,0.5,1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,2,1
34O39PNDK6WYDTB4XU2FVY24Z4RBR1,"['3NAPMVF0ZX3WTHPNVPGZD83NJBS274', '3WAKVUDHUX45DTYPE9Q5JRYF24DU7K', '3WZ36BJEV44DFSMQRLCBJXY92I8BTB']","['A2BQ3DT66XFZ2C', 'A3F9JBVNMQ4ZUV', 'A1ODVXVCDHOVF1']","['760', '51', '166']",19048,18,Objectoriented metrics that predict maintainability,"wei li 0014,sallie m. henry","79
",None,"['https://doi.org/10.1016/0164-1212(93)90077-B', 'http://eprints.cs.vt.edu/archive/00000347/01/TR-93-05.pdf', '10.1016/0164-1212(93)90077-B']","['3', '1', '4', '3', '4']","['1', '1', '1']","['1', '1', '1']","['1', '5', '1', '2', '2']","['{}', '{}', ""Need to re-do hit interface so hit cannot be submitted without ticking all Circle Boxes I think. Highly likely I've submitted a couple without Qualitative/Quantitative ticked.\n\n\n\nMay also need way to hide instructions. Screenspace is precious for high volume turkers.""]","['1123', '1123', '1123']","['http://www.sciencedirect.com/science/article/pii/016412129390077B', 'http://www.sciencedirect.com/science/article/pii/016412129390077B', 'http://www.sciencedirect.com/science/article/pii/016412129390077B?via%3Dihub']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,2,1
34OWYT6U3W52W1TO9NUAQ06NO859IL,"['3OF2M9AATHCAU7ZJ3QEM4OIGAN9KZK', '3QXNC7EIPJJTBQA2VZA58G1EH4P90Y', '3Z3ZLGNNSJIVKHQLVO3AW9PAUIJQ36']","['AZ72Z7VU6TQCN', 'A3F9JBVNMQ4ZUV', 'AMGQ3YEAMC3Z2']","['275', '88', '309']",1907,1,Detection of Duplicate Defect Reports Using Natural Language Processing,"per runeson,magnus alexandersson,oskar nyholm","45
","Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using Natural Language Processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson Mobile Communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.","['doi>10.1109/ICSE.2007.32', 'https://www.researchgate.net/profile/Per_Runeson/publication/4251355_Detection_of_Duplicate_Defect_Reports_Using_Natural_Language_Processing/links/5648d7ac08ae451880aea195.pdf', 'https://doi.org/10.1109/ICSE.2007.32']","['1', '1', '3', '3']","['', '1', '1']","['1', '1', '1']","['2', '3', '1', '2', '2']","['{}', '{}', '{}']","['360', '360', '96']","['http://dl.acm.org/citation.cfm?id=1248882', 'http://dl.acm.org/citation.cfm?id=1248882', 'http://dl.acm.org/citation.cfm?id=1248882']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
34R0BODSP1N7F9PQAER2T53TAJFE5Q,"['32Q90QCQ1T9CJC8N7309JEHYMZGKEU', '333U7HK6IA3CGMO5TNPDCHZ07APJDZ', '3DYGAII7PMW2R6V8TM0H9CSBZOIQP9']","['A3F9JBVNMQ4ZUV', 'AZ72Z7VU6TQCN', 'A314XJY8V1YL12']","['82', '70', '165']",20409,19,Prioritizing Test Cases For Regression Testing,"gregg rothermel,roland h. untch,chengyun chu,mary jean harrold","92
","Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detectiona measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testingthe retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. In this paper, we describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components, 2) techniques that order test cases based on their coverage of code components not previously covered, and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites. Analysis of the data shows that each of the prioritization techniques studied improved the rate of fault detection of test suites, and this improvement occurred even with the least expensive of those techniques. The data also shows, however, that considerable room remains for improvement. The studies highlight several cost-benefit trade-offs among the techniques studied, as well as several opportunities for future work.","['http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1017&context=csearticles', 'DOI: 10.1109/32.962562', 'https://doi.org/10.1109/32.962562']","['1', '1', '1']","['1', '1', '1']","['1', '1', '1']","['2', '1', '2', '1', '2']","['{}', '{}', '{}']","['1076', '1076', '1076']","['http://ieeexplore.ieee.org/abstract/document/962562/', 'http://ieeexplore.ieee.org/abstract/document/962562/', 'http://ieeexplore.ieee.org/abstract/document/962562/']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
34YWR3PJ28YHQ2U68DBI0ZS99HEX0W,"['3570Y55XZQ75NVTVHDROSPHLXU1YGH', '3B2X28YI3X37DUBEV84I8N6VIIKB63']","['A3F9JBVNMQ4ZUV', 'AZ72Z7VU6TQCN']","['185', '174']",3653,1,Software Processes Are Software Too,leon j. osterweil,"62
",None,"['https://pdfs.semanticscholar.org/1bab/9183571e01e140d10b1c840d005d1c7ad872.pdf', 'no doi no']","['3', '1']","['1', '1']","['2', '1']","['1', '2', '4', '2']","['{}', '{}']","['1315', '1315']","['http://dl.acm.org/citation.cfm?id=41766', 'http://dl.acm.org/citation.cfm?id=41766']",1,0,1,0,0,0.5,0,0,0.5,1,0,1,1,0,1,0,0,0.5,1,0,1,1,2,1
359AP8GAGG875FJV2G0411HMGDX7C5,"['39PAAFCODNOSO6KWW23BK9SG558TV9', '3E4GGUZ1T9FKOWW7EJ58HD89PZ52KV', '3WYP994K18F3Q24P50I2GY4W3HUY6D']","['AA2O932UNAZA1', 'AVT70WXOWRHGI', 'A3F9JBVNMQ4ZUV']","['288', '332', '151']",1985,1,Who should fix this bug,"john anvik,lyndon hiew,gail c. murphy","95
","Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.","['10.1145/1134285.1134336', '10.1145/1134285.1134336', 'https://pdfs.semanticscholar.org/faa2/f56920676a677d5961470ea17f28981786cc.pdf']","['1', '4', '1']","['1', '1', '1']","['1', '2', '1']","['2', '4', '1', '2']","['{}', '{}', '{}']","['205', '205', '730']","['http://dl.acm.org/citation.cfm?id=1134336', 'http://dl.acm.org/citation.cfm?id=1134336', 'http://dl.acm.org/citation.cfm?id=1134336']",1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
36AZSFEYZ4OQ60SL6E7PWVR7KA4BVU,"['3570Y55XZQ75NVTVHDROSPHLXUOGYM', '3EICBYG645K2BUIRTRU2SBHIMH9JC9']","['A3F9JBVNMQ4ZUV', 'AZ72Z7VU6TQCN']","['144', '78']",2565,1,Tracking down software bugs using automatic anomaly detection,"sudheendra hangal,monica s. lam","62
","This paper introduces DIDUCE, a practical and effective tool that aids programmers in detecting complex program errors and identifying their root causes. By instrumenting a program and observing its behavior as it runs, DIDUCE dynamically formulates hypotheses of invariants obeyed by the program. DIDUCE hypothesizes the strictest invariants at the beginning, and gradually relaxes the hypothesis as violations are detected to allow for new behavior. The violations reported help users to catch software bugs as soon as they occur. They also give programmers new visibility into the behavior of the programs such as identifying rare corner cases in the program logic or even locating hidden errors that corrupt the program's results.We implemented the DIDUCE system for Java programs and applied it to four programs of significant size and complexity. DIDUCE succeeded in identifying the root causes of programming errors in each of the programs quickly and automatically. In particular, DIDUCE is effective in isolating a timing-dependent bug in a released JSSE (Java Secure Socket Extension) library, which would have taken an experienced programmer days to find. Our experience suggests that detecting and checking program invariants dynamically is a simple and effective methodology for debugging many different kinds of program errors across a wide variety of application domains.","['http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.6659&rep=rep1&type=pdf', 'doi>10.1145/581339.581377']","['1', '1']","['1', '1']","['1', '1']","['2', '1', '2']","['{}', '{}']","['662', '663']","['http://dl.acm.org/citation.cfm?id=581377', 'http://dl.acm.org/citation.cfm?id=581377']",0,0,0.5,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,2,1
36GJS3V78VED6M025BLKUI7A50PJGY,"['3QXNC7EIPJJTBQA2VZA58G1EH4F90O', '3RXPCZQMQQZ8XPIS5OKFE26BQBMG1Z', '3ZSANO2JCGV2D9LR7NTHSIT8GG6SF8']","['A3F9JBVNMQ4ZUV', 'A1ODVXVCDHOVF1', 'AZ72Z7VU6TQCN']","['266', '182', '88']",20079,19,Empirical Validation of ObjectOriented Metrics on Open Source Software for Fault Prediction,"tibor gyimothy,rudolf ferenc,istvan siket","52
","Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug databasecalled Bugzillausing regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle.","['http://flosshub.org/system/files/Gyimothy.pdf', '10.1109/TSE.2005.112', 'DOI: 10.1109/TSE.2005.112']","['1', '3', '2', '1', '2']","['1', '', '1']","['1', '1', '1']","['2', '2', '1', '2']","['{}', '{}', '{}']","['726', '726', '728']","['http://ieeexplore.ieee.org/abstract/document/1542070/', 'http://ieeexplore.ieee.org/abstract/document/1542070/', 'http://ieeexplore.ieee.org/abstract/document/1542070/']",1,0,1,0,0,0.5,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,3,1
